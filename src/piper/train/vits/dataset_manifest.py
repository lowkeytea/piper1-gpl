"""
Data pipeline for voice collection JSONL manifests.

Loads training data from JSONL manifests generated by voice_collection_cli.py
or generate_index_tts_audio_dataset.py. Supports:

1. Multi-speaker training with speaker IDs
2. Multi-emotion training with emotion labels
3. Reference audio loading for speaker encoder training
4. ASR validation filtering

Manifest Format (JSONL):
    {
        "id": "cbx_000001",
        "audio_path": "/path/to/000001.wav",
        "sample_rate": 24000,
        "text": "Hello, how are you?",
        "phonemes": "həˈloʊ haʊ ˈɑːɹ juː",  # optional
        "speaker_id": 100,
        "persona": "M1lkt3a",
        "emotion": "neutral",
        "voice_id": "/path/to/reference.wav",  # reference audio
        "validation": {
            "passed": true,
            "wer": 0.05
        }
    }

Usage:
    from dataset_manifest import ManifestDataModule

    datamodule = ManifestDataModule(
        manifest_paths=["data/voice1_manifest.jsonl", "data/voice2_manifest.jsonl"],
        cache_dir="cache/voice_training",
        espeak_voice="en-us",
        config_path="models/voice_model.onnx.json",
        voice_name="multi_voice",
        sample_rate=22050,
        use_reference_audio=True,  # Load reference audio for speaker encoder
        filter_validation=True,  # Skip failed ASR validation
    )
"""

import glob
import itertools
import json
import logging
import math
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Dict, List, Optional, Sequence, Union

import librosa
import lightning as L
import numpy as np
import torch
try:
    from pysilero_vad import SileroVoiceActivityDetector
except ModuleNotFoundError:  # pragma: no cover
    SileroVoiceActivityDetector = None  # type: ignore[assignment]
from torch import FloatTensor, LongTensor
from torch.utils.data import DataLoader, Dataset, random_split

from piper.config import PhonemeType, PiperConfig
from piper.phoneme_ids import DEFAULT_PHONEME_ID_MAP, phonemes_to_ids

# Try to use C extension first, fall back to subprocess-based phonemizer
try:
    from piper import espeakbridge
    from piper.phonemize_espeak import EspeakPhonemizer
except ImportError:
    logging.getLogger(__name__).info(
        "espeakbridge not available, using subprocess-based phonemizer"
    )
    from piper.phonemize_espeak_subprocess import EspeakPhonemizer

from .mel_processing import spectrogram_torch
from .utils import get_cache_id

_LOGGER = logging.getLogger(__name__)
VAD_SAMPLE_RATE = 16000


@dataclass
class ManifestEntry:
    """Single entry from a JSONL manifest."""

    id: str
    audio_path: Path
    text: str
    sample_rate: int = 24000
    phonemes: Optional[str] = None
    speaker_id: Optional[int] = None
    speaker_name: Optional[str] = None
    emotion: Optional[str] = None
    reference_audio_path: Optional[Path] = None
    validation_passed: bool = True
    wer: Optional[float] = None

    @classmethod
    def from_json(
        cls,
        data: Dict[str, Any],
        manifest_dir: Optional[Path] = None,
    ) -> "ManifestEntry":
        """Create ManifestEntry from JSON dict.

        `manifest_dir` is used to resolve relative audio/reference paths.
        """
        validation = data.get("validation", {})

        # `voice_id` is overloaded in this repo:
        # - voice_collection_cli.py writes a logical speaker name (e.g. adultFemale1)
        # - other pipelines may write a path to reference audio.
        voice_field = data.get("voice_id")
        ref_path: Optional[Path] = None
        speaker_name = data.get("persona")
        if voice_field:
            maybe_path = Path(voice_field)
            if maybe_path.suffix.lower() in {".wav", ".flac", ".mp3", ".ogg"}:
                ref_path = maybe_path
            elif speaker_name is None:
                speaker_name = voice_field

        audio_path = Path(data["audio_path"])
        if manifest_dir is not None and not audio_path.is_absolute():
            audio_path = (manifest_dir / audio_path)

        if ref_path is not None and manifest_dir is not None and not ref_path.is_absolute():
            ref_path = (manifest_dir / ref_path)

        return cls(
            id=data["id"],
            audio_path=audio_path,
            text=data["text"],
            sample_rate=data.get("sample_rate", 24000),
            phonemes=data.get("phonemes"),
            speaker_id=data.get("speaker_id"),
            speaker_name=speaker_name,
            emotion=data.get("emotion"),
            reference_audio_path=ref_path,
            validation_passed=validation.get("passed", True),
            wer=validation.get("wer"),
        )


@dataclass
class CachedManifestUtterance:
    """Cached data paths for a manifest entry."""

    phoneme_ids_path: Path
    audio_norm_path: Path
    audio_spec_path: Path
    reference_spec_path: Optional[Path] = None
    text: Optional[str] = None
    speaker_id: Optional[int] = None
    emotion_id: Optional[int] = None


@dataclass
class ManifestUtteranceTensors:
    """Tensors loaded from cached manifest data."""

    phoneme_ids: LongTensor
    spectrogram: FloatTensor
    audio_norm: FloatTensor
    speaker_id: Optional[LongTensor] = None
    emotion_id: Optional[LongTensor] = None
    reference_spec: Optional[FloatTensor] = None  # For speaker encoder
    text: Optional[str] = None

    @property
    def spec_length(self) -> int:
        return self.spectrogram.size(1)


@dataclass
class ManifestBatch:
    """Batched tensors for training."""

    phoneme_ids: LongTensor
    phoneme_lengths: LongTensor
    spectrograms: FloatTensor
    spectrogram_lengths: LongTensor
    audios: FloatTensor
    audio_lengths: LongTensor
    speaker_ids: Optional[LongTensor] = None
    emotion_ids: Optional[LongTensor] = None
    reference_specs: Optional[FloatTensor] = None
    reference_spec_lengths: Optional[LongTensor] = None


class ManifestDataModule(L.LightningDataModule):
    """
    Lightning DataModule for voice collection manifests.

    Loads and caches data from JSONL manifests for VITS training with
    optional speaker encoder support.
    """

    def __init__(
        self,
        manifest_paths: Union[str, Path, List[Union[str, Path]]],
        cache_dir: Union[str, Path],
        espeak_voice: str,
        config_path: Union[str, Path],
        voice_name: str,
        sample_rate: int = 22050,
        num_symbols: int = 256,
        batch_size: int = 32,
        validation_split: float = 0.1,
        num_test_examples: int = 5,
        filter_length: int = 1024,
        hop_length: int = 256,
        win_length: int = 1024,
        segment_size: int = 8192,
        num_workers: int = 1,
        trim_silence: bool = True,
        keep_seconds_before_silence: float = 0.25,
        keep_seconds_after_silence: float = 0.25,
        # Manifest-specific options
        use_reference_audio: bool = False,
        filter_validation: bool = True,
        min_wer_threshold: Optional[float] = None,
    ) -> None:
        super().__init__()

        # Handle single or multiple manifest paths, with glob expansion.
        if isinstance(manifest_paths, (str, Path)):
            manifest_paths = [manifest_paths]

        expanded: List[Path] = []
        for raw in manifest_paths:
            raw_str = str(raw)
            if any(ch in raw_str for ch in "*?[]"):
                matches = sorted(glob.glob(raw_str))
                if matches:
                    expanded.extend(Path(m) for m in matches)
                else:
                    expanded.append(Path(raw_str))
            else:
                expanded.append(Path(raw_str))

        # De-dupe while preserving order.
        seen: set[Path] = set()
        self.manifest_paths = []
        for p in expanded:
            if p in seen:
                continue
            seen.add(p)
            self.manifest_paths.append(p)

        self.cache_dir = Path(cache_dir)
        self.espeak_voice = espeak_voice
        self.config_path = Path(config_path)
        self.voice_name = voice_name

        self.sample_rate = sample_rate
        self.num_symbols = num_symbols

        self.batch_size = batch_size
        self.validation_split = validation_split
        self.num_test_examples = num_test_examples

        self.filter_length = filter_length
        self.hop_length = hop_length
        self.win_length = win_length
        self.segment_size = segment_size
        self.num_workers = num_workers

        self.trim_silence = trim_silence
        self.keep_seconds_before_silence = keep_seconds_before_silence
        self.keep_seconds_after_silence = keep_seconds_after_silence

        self.use_reference_audio = use_reference_audio
        self.filter_validation = filter_validation
        self.min_wer_threshold = min_wer_threshold

        self.piper_config: Optional[PiperConfig] = None
        self.speaker_id_map: Dict[str, int] = {}
        self.emotion_id_map: Dict[str, int] = {}
        self.num_speakers = 0
        self.num_emotions = 0

    def _load_manifests(self) -> List[ManifestEntry]:
        """Load and merge all manifest files."""
        entries = []

        for manifest_path in self.manifest_paths:
            if not manifest_path.exists():
                _LOGGER.warning("Manifest not found: %s", manifest_path)
                continue

            _LOGGER.info("Loading manifest: %s", manifest_path)
            with open(manifest_path, "r", encoding="utf-8") as f:
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    if not line:
                        continue

                    try:
                        data = json.loads(line)
                        entry = ManifestEntry.from_json(data, manifest_dir=manifest_path.parent)

                        # Filter by validation
                        if self.filter_validation and not entry.validation_passed:
                            continue

                        if (
                            self.min_wer_threshold is not None
                            and entry.wer is not None
                            and entry.wer > self.min_wer_threshold
                        ):
                            continue

                        # Check audio exists (after resolving relative paths)
                        if not entry.audio_path.exists():
                            _LOGGER.warning(
                                "Audio not found: %s (manifest line %d)",
                                entry.audio_path,
                                line_num,
                            )
                            continue

                        entries.append(entry)

                    except (json.JSONDecodeError, KeyError) as e:
                        _LOGGER.warning(
                            "Failed to parse manifest line %d: %s",
                            line_num,
                            e,
                        )

        _LOGGER.info("Loaded %d valid entries from %d manifests",
                     len(entries), len(self.manifest_paths))
        return entries

    def _build_id_maps(self, entries: List[ManifestEntry]) -> None:
        """Build speaker and emotion ID maps from entries."""
        for entry in entries:
            # Speaker mapping
            if entry.speaker_name and entry.speaker_name not in self.speaker_id_map:
                self.speaker_id_map[entry.speaker_name] = len(self.speaker_id_map)

            # Use speaker_id from manifest if no name
            if entry.speaker_id is not None and not entry.speaker_name:
                speaker_key = f"speaker_{entry.speaker_id}"
                if speaker_key not in self.speaker_id_map:
                    self.speaker_id_map[speaker_key] = entry.speaker_id

            # Emotion mapping
            if entry.emotion and entry.emotion not in self.emotion_id_map:
                self.emotion_id_map[entry.emotion] = len(self.emotion_id_map)

        self.num_speakers = max(len(self.speaker_id_map), 1)
        self.num_emotions = len(self.emotion_id_map)

        _LOGGER.info("Speaker ID map: %s", self.speaker_id_map)
        _LOGGER.info("Emotion ID map: %s", self.emotion_id_map)

    def _write_piper_config(self) -> None:
        """Write piper config JSON file."""
        if self.piper_config is None:
            self.piper_config = PiperConfig(
                num_symbols=self.num_symbols,
                num_speakers=self.num_speakers,
                sample_rate=self.sample_rate,
                espeak_voice=self.espeak_voice,
                phoneme_id_map=DEFAULT_PHONEME_ID_MAP,
                phoneme_type=PhonemeType.ESPEAK,
                piper_version="1.3.0",
            )

            if self.speaker_id_map:
                self.piper_config.speaker_id_map = self.speaker_id_map
            if self.emotion_id_map:
                self.piper_config.emotion_id_map = self.emotion_id_map

        self.config_path.parent.mkdir(parents=True, exist_ok=True)
        with open(self.config_path, "w", encoding="utf-8") as f:
            json.dump(self.piper_config.to_dict(), f, ensure_ascii=False, indent=2)

    def prepare_data(self) -> None:
        """Cache phonemes, audio, and spectrograms."""
        self.cache_dir.mkdir(parents=True, exist_ok=True)

        # Check for cache completion marker to skip expensive re-processing
        cache_marker = self.cache_dir / ".cache_complete"
        cache_config = self.cache_dir / ".cache_config.json"

        # Build a config signature to detect changes that would invalidate cache
        config_sig = {
            "manifest_paths": [str(p) for p in self.manifest_paths],
            "sample_rate": self.sample_rate,
            "filter_length": self.filter_length,
            "hop_length": self.hop_length,
            "win_length": self.win_length,
            "trim_silence": self.trim_silence,
            "filter_validation": self.filter_validation,
            "use_reference_audio": self.use_reference_audio,
        }

        # Check if cache is valid
        if cache_marker.exists() and cache_config.exists():
            try:
                existing_config = json.loads(cache_config.read_text())
                if existing_config == config_sig:
                    _LOGGER.info(
                        "Cache marker found at %s - skipping prepare_data. "
                        "Delete this file to force re-processing.",
                        cache_marker,
                    )
                    # Still need to load manifests to build ID maps for setup()
                    entries = self._load_manifests()
                    self._build_id_maps(entries)
                    self._write_piper_config()
                    return
                else:
                    _LOGGER.info("Cache config changed, re-processing...")
            except (json.JSONDecodeError, KeyError):
                _LOGGER.info("Invalid cache config, re-processing...")

        entries = self._load_manifests()
        if not entries:
            raise ValueError(
                "No valid manifest entries loaded. "
                "Check --data.manifest_paths and that audio_path values resolve correctly."
            )
        self._build_id_maps(entries)
        self._write_piper_config()

        phonemizer = EspeakPhonemizer()
        vad = None
        if self.trim_silence:
            if SileroVoiceActivityDetector is None:
                raise ModuleNotFoundError(
                    "trim_silence=True requires pysilero-vad. "
                    "Install `pysilero-vad` or set trim_silence=false."
                )
            vad = SileroVoiceActivityDetector()

        processed = 0
        for entry in entries:
            cache_id = self._get_cache_id(entry)

            # Text
            text_path = self.cache_dir / f"{cache_id}.txt"
            if not text_path.exists():
                text_path.write_text(entry.text, encoding="utf-8")

            # Phonemes
            phonemes_path = self.cache_dir / f"{cache_id}.phonemes.txt"
            phoneme_ids_path = self.cache_dir / f"{cache_id}.phonemes.pt"

            phonemes: Optional[List[List[str]]] = None
            if not phoneme_ids_path.exists():
                # Try to use phonemes from manifest
                if entry.phonemes:
                    # Convert IPA string to phoneme list
                    phonemes = [[c for c in entry.phonemes]]
                else:
                    # Generate with espeak
                    phonemes = phonemizer.phonemize(self.espeak_voice, entry.text)

                if not phonemes_path.exists():
                    with open(phonemes_path, "w", encoding="utf-8") as f:
                        for sentence_phonemes in phonemes:
                            print("".join(sentence_phonemes), file=f)

                phoneme_ids = list(
                    itertools.chain(
                        *(
                            phonemes_to_ids(sentence_phonemes)
                            for sentence_phonemes in phonemes
                        )
                    )
                )
                torch.save(torch.LongTensor(phoneme_ids), phoneme_ids_path)

            # Normalized audio
            norm_audio_path = self.cache_dir / f"{cache_id}.audio.pt"
            audio_norm_tensor: Optional[torch.Tensor] = None

            if not norm_audio_path.exists():
                audio_norm_array, _ = librosa.load(
                    path=entry.audio_path, sr=self.sample_rate, mono=True
                )

                if self.trim_silence:
                    audio_16khz_array, _ = librosa.load(
                        path=entry.audio_path, sr=VAD_SAMPLE_RATE, mono=True
                    )
                    audio_norm_array = self._trim_silence(
                        audio_norm_array, audio_16khz_array, vad
                    )

                audio_norm_tensor = torch.FloatTensor(audio_norm_array)
                torch.save(audio_norm_tensor, norm_audio_path)

            # Spectrogram
            audio_spec_path = self.cache_dir / f"{cache_id}.spec.pt"
            if not audio_spec_path.exists():
                if audio_norm_tensor is None:
                    audio_norm_tensor = torch.load(norm_audio_path)

                torch.save(
                    spectrogram_torch(
                        y=audio_norm_tensor.unsqueeze(0),
                        n_fft=self.filter_length,
                        sampling_rate=self.sample_rate,
                        hop_size=self.hop_length,
                        win_size=self.win_length,
                        center=False,
                    ).squeeze(0),
                    audio_spec_path,
                )

            # Reference audio spectrogram (for speaker encoder)
            if self.use_reference_audio and entry.reference_audio_path:
                ref_spec_path = self.cache_dir / f"{cache_id}.ref_spec.pt"
                if not ref_spec_path.exists() and entry.reference_audio_path.exists():
                    ref_audio, _ = librosa.load(
                        path=entry.reference_audio_path,
                        sr=self.sample_rate,
                        mono=True,
                    )
                    ref_tensor = torch.FloatTensor(ref_audio)
                    torch.save(
                        spectrogram_torch(
                            y=ref_tensor.unsqueeze(0),
                            n_fft=self.filter_length,
                            sampling_rate=self.sample_rate,
                            hop_size=self.hop_length,
                            win_size=self.win_length,
                            center=False,
                        ).squeeze(0),
                        ref_spec_path,
                    )

            processed += 1
            if processed % 100 == 0:
                _LOGGER.info("Processed %d/%d entries...", processed, len(entries))

        _LOGGER.info("Processed %d entries total", processed)

        # Write cache completion marker
        cache_config.write_text(json.dumps(config_sig), encoding="utf-8")
        cache_marker.write_text(
            f"Cache completed with {processed} entries.\n"
            f"Delete this file to force re-processing.\n",
            encoding="utf-8",
        )
        _LOGGER.info("Cache marker written to %s", cache_marker)

    def _get_cache_id(self, entry: ManifestEntry) -> str:
        """Generate cache ID for manifest entry."""
        speaker_id = None
        if entry.speaker_name and entry.speaker_name in self.speaker_id_map:
            speaker_id = self.speaker_id_map[entry.speaker_name]
        elif entry.speaker_id is not None:
            speaker_id = entry.speaker_id

        return get_cache_id(
            hash(entry.id),
            entry.text,
            speaker_id=speaker_id,
        )

    def setup(self, stage: str) -> None:
        """Set up datasets for training/validation/test."""
        assert self.piper_config is not None

        entries = self._load_manifests()
        if not entries:
            raise ValueError(
                "No valid manifest entries loaded. "
                "Nothing to train on; check manifest_paths/audio_path resolution."
            )
        all_utts: List[CachedManifestUtterance] = []

        for entry in entries:
            cache_id = self._get_cache_id(entry)

            phoneme_ids_path = self.cache_dir / f"{cache_id}.phonemes.pt"
            audio_norm_path = self.cache_dir / f"{cache_id}.audio.pt"
            audio_spec_path = self.cache_dir / f"{cache_id}.spec.pt"

            # Skip if cache files don't exist
            if not all(p.exists() for p in [phoneme_ids_path, audio_norm_path, audio_spec_path]):
                continue

            # Get IDs
            speaker_id = None
            if entry.speaker_name and entry.speaker_name in self.speaker_id_map:
                speaker_id = self.speaker_id_map[entry.speaker_name]
            elif entry.speaker_id is not None:
                speaker_id = entry.speaker_id

            emotion_id = None
            if entry.emotion and entry.emotion in self.emotion_id_map:
                emotion_id = self.emotion_id_map[entry.emotion]

            # Reference spectrogram path
            ref_spec_path = None
            if self.use_reference_audio:
                ref_spec_path = self.cache_dir / f"{cache_id}.ref_spec.pt"
                if not ref_spec_path.exists():
                    ref_spec_path = None

            all_utts.append(
                CachedManifestUtterance(
                    phoneme_ids_path=phoneme_ids_path,
                    audio_norm_path=audio_norm_path,
                    audio_spec_path=audio_spec_path,
                    reference_spec_path=ref_spec_path,
                    text=entry.text,
                    speaker_id=speaker_id,
                    emotion_id=emotion_id,
                )
            )

        full_dataset = ManifestDataset(all_utts, self.use_reference_audio)

        valid_set_size = int(len(full_dataset) * self.validation_split)
        train_set_size = len(full_dataset) - valid_set_size - self.num_test_examples
        self.train_dataset, self.test_dataset, self.val_dataset = random_split(
            full_dataset, [train_set_size, self.num_test_examples, valid_set_size]
        )

        _LOGGER.info(
            "Dataset split: train=%d, val=%d, test=%d",
            train_set_size, valid_set_size, self.num_test_examples,
        )

    def train_dataloader(self) -> DataLoader:
        return DataLoader(
            self.train_dataset,
            collate_fn=ManifestCollate(
                is_multispeaker=(self.num_speakers > 1),
                is_multiemotion=(self.num_emotions > 0),
                use_reference_audio=self.use_reference_audio,
                segment_size=self.segment_size,
            ),
            batch_size=self.batch_size,
            num_workers=self.num_workers,
            shuffle=True,
        )

    def val_dataloader(self) -> DataLoader:
        return DataLoader(
            self.val_dataset,
            collate_fn=ManifestCollate(
                is_multispeaker=(self.num_speakers > 1),
                is_multiemotion=(self.num_emotions > 0),
                use_reference_audio=self.use_reference_audio,
                segment_size=self.segment_size,
            ),
            batch_size=self.batch_size,
            num_workers=self.num_workers,
        )

    def test_dataloader(self) -> DataLoader:
        return DataLoader(
            self.test_dataset,
            collate_fn=ManifestCollate(
                is_multispeaker=(self.num_speakers > 1),
                is_multiemotion=(self.num_emotions > 0),
                use_reference_audio=self.use_reference_audio,
                segment_size=self.segment_size,
            ),
            batch_size=self.batch_size,
            num_workers=self.num_workers,
        )

    def _trim_silence(
        self,
        audio_original_array: np.ndarray,
        audio_16khz_array: np.ndarray,
        vad: SileroVoiceActivityDetector,
        threshold: float = 0.2,
    ) -> np.ndarray:
        """Trim silence from audio using VAD."""
        vad.reset()

        first_chunk: Optional[int] = None
        last_chunk: Optional[int] = None

        samples_per_chunk = vad.chunk_samples()
        seconds_per_chunk: float = samples_per_chunk / VAD_SAMPLE_RATE
        num_chunks = len(audio_16khz_array) // samples_per_chunk

        for chunk_idx in range(num_chunks):
            chunk_offset = chunk_idx * samples_per_chunk
            chunk = audio_16khz_array[chunk_offset : chunk_offset + samples_per_chunk]
            if len(chunk) < samples_per_chunk:
                continue

            # pysilero-vad API changed across versions; support both.
            if hasattr(vad, "process_array"):
                prob = vad.process_array(chunk)  # type: ignore[attr-defined]
            elif hasattr(vad, "process_samples"):
                prob = vad.process_samples(chunk.tolist())  # type: ignore[attr-defined]
            else:
                prob = vad.process_chunk(chunk.astype(np.float32).tobytes())  # type: ignore[attr-defined]
            is_speech = prob >= threshold

            if is_speech:
                if first_chunk is None:
                    first_chunk = chunk_idx
                else:
                    last_chunk = chunk_idx

        if first_chunk is not None and last_chunk is not None:
            num_original_samples = len(audio_original_array)
            audio_seconds = len(audio_16khz_array) / VAD_SAMPLE_RATE

            first_sec = first_chunk * seconds_per_chunk
            first_sec = max(0, first_sec - self.keep_seconds_before_silence)
            first_sample = int(
                math.floor(num_original_samples * (first_sec / audio_seconds))
            )

            last_sec = (last_chunk + 1) * seconds_per_chunk
            last_sec = min(audio_seconds, last_sec + self.keep_seconds_after_silence)
            last_sample = int(
                math.ceil(num_original_samples * (last_sec / audio_seconds))
            )

            return audio_original_array[first_sample:last_sample]

        return audio_original_array


class ManifestDataset(Dataset):
    """Dataset for cached manifest utterances."""

    def __init__(
        self,
        utts: List[CachedManifestUtterance],
        use_reference_audio: bool = False,
    ):
        self.utts = utts
        self.use_reference_audio = use_reference_audio

    def __len__(self) -> int:
        return len(self.utts)

    def __getitem__(self, idx: int) -> ManifestUtteranceTensors:
        utt = self.utts[idx]

        reference_spec = None
        if self.use_reference_audio and utt.reference_spec_path:
            reference_spec = torch.load(utt.reference_spec_path)

        return ManifestUtteranceTensors(
            phoneme_ids=torch.load(utt.phoneme_ids_path),
            audio_norm=torch.load(utt.audio_norm_path),
            spectrogram=torch.load(utt.audio_spec_path),
            speaker_id=(
                LongTensor([utt.speaker_id]) if utt.speaker_id is not None else None
            ),
            emotion_id=(
                LongTensor([utt.emotion_id]) if utt.emotion_id is not None else None
            ),
            reference_spec=reference_spec,
            text=utt.text,
        )


class ManifestCollate:
    """Collate function for manifest batches."""

    def __init__(
        self,
        is_multispeaker: bool,
        is_multiemotion: bool,
        use_reference_audio: bool,
        segment_size: int,
    ):
        self.is_multispeaker = is_multispeaker
        self.is_multiemotion = is_multiemotion
        self.use_reference_audio = use_reference_audio
        self.segment_size = segment_size

    def __call__(self, utterances: Sequence[ManifestUtteranceTensors]) -> ManifestBatch:
        num_utterances = len(utterances)
        assert num_utterances > 0, "No utterances"

        max_phonemes_length = 0
        max_spec_length = 0
        max_audio_length = 0
        max_ref_spec_length = 0
        num_mels = 0

        # Determine lengths
        for utt in utterances:
            max_phonemes_length = max(max_phonemes_length, utt.phoneme_ids.size(0))
            max_spec_length = max(max_spec_length, utt.spectrogram.size(1))
            max_audio_length = max(max_audio_length, utt.audio_norm.size(0))
            num_mels = utt.spectrogram.size(0)

            if utt.reference_spec is not None:
                max_ref_spec_length = max(max_ref_spec_length, utt.reference_spec.size(1))

        max_audio_length = max(max_audio_length, self.segment_size)

        # Create padded tensors
        phonemes_padded = LongTensor(num_utterances, max_phonemes_length).zero_()
        spec_padded = FloatTensor(num_utterances, num_mels, max_spec_length).zero_()
        audio_padded = FloatTensor(num_utterances, 1, max_audio_length).zero_()

        phoneme_lengths = LongTensor(num_utterances)
        spec_lengths = LongTensor(num_utterances)
        audio_lengths = LongTensor(num_utterances)

        speaker_ids: Optional[LongTensor] = None
        if self.is_multispeaker:
            speaker_ids = LongTensor(num_utterances)

        emotion_ids: Optional[LongTensor] = None
        if self.is_multiemotion:
            emotion_ids = LongTensor(num_utterances)

        reference_specs: Optional[FloatTensor] = None
        reference_spec_lengths: Optional[LongTensor] = None
        if self.use_reference_audio and max_ref_spec_length > 0:
            reference_specs = FloatTensor(num_utterances, num_mels, max_ref_spec_length).zero_()
            reference_spec_lengths = LongTensor(num_utterances)

        # Sort by decreasing spectrogram length
        sorted_utterances = sorted(
            utterances, key=lambda u: u.spectrogram.size(1), reverse=True
        )

        for utt_idx, utt in enumerate(sorted_utterances):
            phoneme_length = utt.phoneme_ids.size(0)
            spec_length = utt.spectrogram.size(1)
            audio_length = utt.audio_norm.size(0)

            phonemes_padded[utt_idx, :phoneme_length] = utt.phoneme_ids
            phoneme_lengths[utt_idx] = phoneme_length

            spec_padded[utt_idx, :, :spec_length] = utt.spectrogram
            spec_lengths[utt_idx] = spec_length

            audio_padded[utt_idx, :, :audio_length] = utt.audio_norm
            audio_lengths[utt_idx] = audio_length

            if utt.speaker_id is not None and speaker_ids is not None:
                speaker_ids[utt_idx] = utt.speaker_id

            if utt.emotion_id is not None and emotion_ids is not None:
                emotion_ids[utt_idx] = utt.emotion_id

            if utt.reference_spec is not None and reference_specs is not None:
                ref_len = utt.reference_spec.size(1)
                reference_specs[utt_idx, :, :ref_len] = utt.reference_spec
                reference_spec_lengths[utt_idx] = ref_len

        return ManifestBatch(
            phoneme_ids=phonemes_padded,
            phoneme_lengths=phoneme_lengths,
            spectrograms=spec_padded,
            spectrogram_lengths=spec_lengths,
            audios=audio_padded,
            audio_lengths=audio_lengths,
            speaker_ids=speaker_ids,
            emotion_ids=emotion_ids,
            reference_specs=reference_specs,
            reference_spec_lengths=reference_spec_lengths,
        )
